import streamlit as st
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import os
import io
from PIL import Image
from wordcloud import WordCloud
from collections import Counter
import logging
import sys
from langdetect import detect
import unicodedata
import regex
import matplotlib.font_manager as fm
import pandas as pd

# Try to import indic_tokenize, with fallback
try:
    from indicnlp.tokenize import indic_tokenize
    INDIC_NLP_AVAILABLE = True
except ImportError:
    INDIC_NLP_AVAILABLE = False
    logging.warning("indic-nlp-library not available. Falling back to regex tokenization for Indic languages.")

# Try to import grapheme library for better Indic script handling
try:
    import grapheme
    GRAPHEME_AVAILABLE = True
except ImportError:
    GRAPHEME_AVAILABLE = False
    logging.warning("grapheme library not available. Falling back to regex for grapheme clustering.")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Ensure NLTK data is downloaded
def ensure_nltk_data():
    try:
        nltk.data.find('tokenizers/punkt')
        nltk.data.find('corpora/stopwords')
    except LookupError:
        logger.info("Downloading NLTK data...")
        nltk.download('punkt')
        nltk.download('stopwords')
        logger.info("NLTK data downloaded successfully.")

# Load stopwords for different languages
def load_stopwords():
    """Load stopwords for different languages.
    
    Returns:
        dict: Dictionary with language codes as keys and sets of stopwords as values.
    """
    stopword_dict = {}
    
    # English stopwords from NLTK
    try:
        stopword_dict['english'] = set(stopwords.words('english'))
    except:
        ensure_nltk_data()
        stopword_dict['english'] = set(stopwords.words('english'))
    
    # Hindi stopwords (minimal curated list)
    stopword_dict['hindi'] = {
        'рдХрд╛', 'рдХреЗ', 'рдХреА', 'рд╣реИ', 'рдореЗрдВ', 'рд╕реЗ', 'рд╣реИрдВ', 'рдХреЛ', 'рдкрд░', 'рдЗрд╕', 'рд╣реЛрддрд╛', 'рдХрд┐', 'рдЬреЛ', 'рдХрд░', 'рдореЗ', 
        'рдЧрдпрд╛', 'рдХрд░рдиреЗ', 'рдХрд┐рдпрд╛', 'рд▓рд┐рдпреЗ', 'рдЕрдкрдиреЗ', 'рдиреЗ', 'рдмрдиреА', 'рдирд╣реАрдВ', 'рддреЛ', 'рд╣реА', 'рдпрд╛', 'рдПрд╡рдВ', 'рджрд┐рдпрд╛', 'рд╣реЛ', 
        'рдЗрд╕рдХрд╛', 'рдерд╛', 'рджреНрд╡рд╛рд░рд╛', 'рд╣реБрдЖ', 'рддрдХ', 'рд╕рд╛рде', 'рдХрд░рддрд╛', 'рд╣реБрдИ', 'рдПрдХ', 'рдФрд░', 'рдпрд╣', 'рд░рд╣рд╛', 'рд╣реБрдП', 'рдереЗ', 
        'рдХрд░реЗрдВ', 'рдЗрд╕рдХреЗ', 'рдереА', 'рдЙрд╕', 'рд╣реВрдБ', 'рдЬрд╛', 'рдирд╛', 'рдЙрди', 'рд╡рд╣', 'рднреА', 'рд╡реЗ', 'рдереА', 'рдЬрдм', 'рд╣реЛрддреЗ', 'рдХреЛрдИ', 
        'рд╣рдо', 'рдЖрдк', 'рдлрд┐рд░', 'рдмрд╣реБрдд', 'рдХрд╣рд╛', 'рд╡рд╛рд▓реЗ', 'рдЬреИрд╕реЗ', 'рд╕рднреА', 'рдХреБрдЫ', 'рдХреНрдпрд╛', 'рдЕрдм', 'рдЙрдирдХреЗ', 'рдЗрд╕реА', 'рд░рд╣реЗ', 
        'рдЙрдирдХреА', 'рдЙрдирдХрд╛', 'рдЕрдкрдиреА', 'рдЙрд╕рдХреЗ', 'рддрдерд╛', 'рджреЛ', 'рд╡рд╣рд╛рдВ', 'рдЧрдпреЗ', 'рдмрдбрд╝реЗ', 'рд╡рд░реНрдЧ', 'рддрд░рд╣', 'рд░рд╣реА', 'рдХрд┐рд╕реА', 
        'рдРрд╕реЗ', 'рд░рдЦреЗрдВ', 'рдЕрдкрдирд╛', 'рдЙрд╕реЗ', 'рдЬрд┐рд╕рдореЗрдВ', 'рдХрд┐рдиреНрд╣реЗрдВ', 'рд░реВрдк', 'рдХрд┐рдиреНрд╣реЛрдВрдиреЗ', 'рдХрд┐рдпрд╛', 'рд▓реЗрдХрд┐рди', 'рдХрдо', 'рд╣реЛрддреА', 
        'рдЕрдзрд┐рдХ', 'рдЕрдм', 'рдФрд░', 'рд╡рд░реНрд╖', 'рдпрджрд┐', 'рд╣реБрдпреЗ', 'рдЗрд╕рд▓рд┐рдП', 'рд░рдЦрд╛', 'рдХрд┐рдпреЗ', 'рдЕрдиреНрдп', 'рднрд╛рдЧ', 'рдЙрдиреНрд╣реЗрдВ', 'рдЧрдпреА', 
        'рдкреНрд░рддрд┐', 'рдХреБрд▓', 'рдПрд╕', 'рд░рд╣рддреА', 'рдЗрд╕рдореЗрдВ', 'рдЬрд┐рд╕', 'рдкреНрд░рдХрд╛рд░', 'рдЖрджрд┐', 'рдЗрди', 'рдЕрднреА', 'рдЖрдЬ', 'рдХрд▓', 'рдЬрд┐рдиреНрд╣реЗрдВ', 
        'рдЬрд┐рдиреНрд╣реЛрдВрдиреЗ', 'рддрдм', 'рдЙрд╕рдХреА', 'рдЙрд╕рдХрд╛', 'рдпрд╣рд╛рдБ', 'рдЗрд╕рдХреА', 'рд╕рдХрддреА', 'рдЗрд╕реЗ', 'рдЬрд┐рд╕рдХреЗ', 'рд╕рдмрд╕реЗ', 'рд╣реЛрдиреЗ', 'рдмрд╛рдд', 
        'рдпрд╣реА', 'рд╡рд╣реА', 'рджрд┐рди', 'рдХрд╣рддреЗ', 'рдЕрдкрдиреА', 'рдХрдИ', 'рддрд░рдл', 'рдмрд╛рдж', 'рд▓рд┐рдП', 'рд░рдЦ', 'рд░рдЦреА', 'рдЙрдиреНрд╣реЛрдВрдиреЗ', 'рд╡рд╣реАрдВ', 
        'рдЙрдиреНрд╣реАрдВ', 'рдЬрд╛', 'рдЬрд╛рддрд╛', 'рдЬрд╛рддреА', 'рдмрд╛рд╣рд░', 'рдЖ', 'рдЖрддрд╛', 'рдЖрддреА', 'рд╡рд╛рд▓рд╛', 'рд╡рд╛рд▓реА', 'рд╣рд░', 'рд╣рд░', 'рдЬрд╛рдП', 
        'рдЬрд╛рдПрдЧрд╛', 'рдЬрд╛рдПрдБрдЧреЗ', 'рдЬрд╛рдУ', 'рдЖрдУ', 'рдЖрдПрдЧрд╛', 'рдЖрдПрдБрдЧреЗ', 'рд╡рд╣рд╛рдБ', 'рдЬрд╣рд╛рдБ', 'рд╡рдЧрд╝реИрд░рд╣', 'рдиреАрдЪреЗ', 'рдКрдкрд░', 'рд╡рд╛рд▓реЗ', 
        'рд╕рд╛рд░реЗ', 'рд╕рд╛рд░реА', 'рдЕрдВрджрд░', 'рдорд╛рдирд╛', 'рдорд╛рдиреА', 'рдорд╛рдиреЛ', 'рдЕрдЪреНрдЫрд╛', 'рдЕрдЪреНрдЫреА', 'рдЕрдЪреНрдЫреЗ', 'рд▓реЗ', 'рд▓реЛ', 'рджреЗ', 'рджреЛ', 
        'рдЙрд╕рдХреЛ', 'рдЙрд╕рдХреА', 'рдЙрд╕рдХреЗ', 'рдЙрд╕рд╕реЗ', 'рдЙрд╕рдиреЗ', 'рдЙрд╕рдореЗрдВ', 'рдЙрд╕реА', 'рдЙрдиреНрд╣реАрдВ', 'рдЙрдиреНрд╣реЗрдВ', 'рдЙрдирд╕реЗ', 'рдЙрдирдХреЛ', 'рдЙрдирдореЗрдВ'
    }
    
    # Assamese stopwords (minimal curated list)
    stopword_dict['assamese'] = {
        'ржЖрз░рзБ', 'ржПржЗ', 'ржПржЯрж╛', 'ржПржирзЗ', 'рждрж╛рз░', 'ржирж╛ржЗ', 'рж╣ржпрж╝', 'рж╣рзИ', 'рж╣рж▓', 'рж╣ржм', 'ржХрз░рж┐', 'ржХрз░рж╛', 'ржХрз░рзЗ', 'ржХрз░рж┐ржм', 
        'ржХрз░рж┐ржмрж╛', 'ржХрз░рж┐рж▓рзЗ', 'ржиржХрз░рзЗ', 'ржиржХрз░рж╛', 'ржЖржЫрзЗ', 'ржЖржЫрж┐рж▓', 'ржерж╛ржХрзЗ', 'ржерж╛ржХрж┐ржм', 'ржпрж╛ржпрж╝', 'ржпрж╛ржм', 'ржпрж╛ржУржХ', 'ржЖрж╣рзЗ', 
        'ржЖрж╣рж┐ржм', 'ржЖрж╣рж┐ржмрж╛', 'ржкрж╛рз░рзЗ', 'ржкрж╛рз░рж┐ржм', 'рж▓рж╛ржЧрзЗ', 'рж▓рж╛ржЧрж┐ржм', 'рж▓рж╛ржЧрж┐рж▓', 'рж╣рзЛрз▒рж╛', 'рж╣рзЛрз▒рж╛рз░', 'рж╣рзЛрз▒рж╛ржЗ', 'рждрзЗржУржБ', 
        'рждрзЗржУржБрз░', 'рждрзЗржУржБржХ', 'ржоржЗ', 'ржорзЛржХ', 'ржорзЛрз░', 'ржЖржорж┐', 'ржЖржорж╛ржХ', 'ржЖржорж╛рз░', 'рждрзБржорж┐', 'рждрзЛржорж╛ржХ', 'рждрзЛржорж╛рз░', 'рждрзЗржУржБрж▓рзЛржХ', 
        'рждрзЗржУржБрж▓рзЛржХрз░', 'рждрзЗржУржБрж▓рзЛржХржХ', 'ржХрж┐', 'ржХрж┐ржпрж╝', 'ржХрзЗржирзЗ', 'ржХрзЗржирзЗржХрзИ', 'ржпрж╝рж╛рз░', 'ржХржд', 'ржХрж┐ржорж╛ржи', 'ржХрзЛржи', 'ржХрзЛржирзЗ', 
        'ржпрж┐', 'ржпрж┐ржпрж╝рзЗ', 'ржпрж╛ржХ', 'ржпрж╛рз░', 'ржпрж┐рж╣рзЗрждрзБ', 'ржпрж┐ржЦрж┐ржирж┐', 'ржпрж┐ржорж╛ржи', 'ржпрж┐ржЯрзЛ', 'ржпрж┐ржмрзЛрз░', 'ржпржд', 'ржпрждржмрзЛрз░', 'ржпржерж╛', 
        'ржпржжрж┐', 'ржпржжрж┐ржУ', 'ржпржжрж┐рж╣рзЗ', 'ржпрж╛рждрзЗ', 'ржпрж┐рж╣ржд', 'рж╕рзЗржЗ', 'рж╕рзЗржЗржЯрзЛ', 'рж╕рзЗржЗржмрзЛрз░', 'рждрзЗржирзЗ', 'рждрзЗржирзЗржХрзБрз▒рж╛', 'рждрзЗрждрж┐ржпрж╝рж╛', 
        'рждрж╛ржд', 'рждрж╛ржХ', 'рждрж╛рз░', 'рждрж╛ржЗ', 'рждрж┐ржирж┐', 'рждрж┐ржирж┐ржУржЯрж╛', 'рждрж┐ржирж┐ржЯрж╛', 'рж╕рж┐', 'рж╕рж┐рж╣ржБржд', 'рж╕рж┐рж╣ржБрждрз░', 'рж╕рж┐рж╣ржБрждржХ', 
        'ржЖрз░рзБ', 'ржмрж╛', 'ржмрж╛рз░рзБ', 'ржирждрзБржмрж╛', 'ржпрж╛рждрзЗ', 'рждржерж╛ржкрж┐', 'ржХрж┐ржирзНрждрзБ', 'ржпржжрж┐ржУ', 'рждрзЗржирзНрждрзЗ', 'рждрзЗрждрж┐ржпрж╝рж╛рж╣рж▓рзЗ', 'ржирж╛ржЗржмрж╛', 
        'ржирж╣рж▓рзЗ', 'ржирж╣ржпрж╝', 'ржирж╛ржЗржХрж┐ржпрж╝рж╛', 'ржирзЛрж╣рзЛрз▒рж╛', 'ржирзЛрж╣рзЛрз▒рж╛ржХрзИ', 'ржирзЛрж╣рзЛрз▒рж╛рж▓рзИржХрзЗ', 'рж╣рзЗ', 'рж╣ржпрж╝', 'рж╣ржпрж╝рждрзЛ', 'рж╣ржмрж▓рж╛', 
        'рж╣рж▓рзЗ', 'рж╣рж▓рзЛ', 'рж╣рзИржЫрзЗ', 'рж╣рзИржЫрж┐рж▓', 'рж╣рзИ', 'рж╣рзЛрз▒рж╛', 'рж╣рзЛрз▒рж╛ржЗ', 'рж╣рзЛрз▒рж╛ржд', 'рж╣рзЛрз▒рж╛рз░', 'ржирж╣ржпрж╝', 'ржирж╣ржм', 'ржирж╣рж▓', 
        'ржирж╣рзЗ', 'ржирзЛрж╣рзЛрз▒рж╛', 'ржирж╛ржЫрж┐рж▓', 'ржирж╛ржЗ', 'ржирж╛ржЗржХрж┐ржпрж╝рж╛', 'ржирж┐ржЪрж┐ржирж╛', 'ржирж┐ржЪрж┐ржирзЗ', 'ржХрз░ржХ', 'ржХрз░рж╛', 'ржХрз░рж┐', 'ржХрз░рж┐ржм', 
        'ржХрз░рж┐ржмрж▓рзИ', 'ржХрз░рж┐ржмрж╛', 'ржХрз░рж┐ржмрзЗ', 'ржХрз░рж┐ржо', 'ржХрз░рж┐ржпрж╝рзЗ', 'ржХрз░рж┐рж▓рзЗ', 'ржХрз░рж┐рж▓рзЗржЗ', 'ржХрз░рж┐рж▓рзЗржУ', 'ржХрз░рж┐рж▓рзЛржБ', 'ржХрз░рзЗ', 
        'ржХрз░рзЛ', 'ржХрз░рзЛржБ', 'ржХрз░рзЛржБрждрзЗ', 'ржХрз░рзЛржБрждрзЗржЗ', 'ржиржХрз░рж╛', 'ржиржХрз░рж┐ржм', 'ржиржХрз░рж┐ржмрж╛', 'ржиржХрз░рж┐рж▓рзЗ', 'ржиржХрз░рзЗ', 'ржиржХрз░рзЛ', 
        'ржиржХрз░рзЛржБ', 'ржХрз░рж╛ржУржХ', 'ржХрз░рж╛ржЗ', 'ржХрз░рж╛ржЗржЫрзЗ', 'ржХрз░рж╛ржЗржЫрж┐рж▓', 'ржХрз░рж╛ржм', 'ржХрз░рж╛ржмрж╛', 'ржХрз░рж╛ржмрзЗ', 'ржХрз░рж╛ржо', 'ржХрз░рж╛рж▓рзЗ', 
        'ржХрз░рж╛рж▓рзЗржУ', 'ржХрз░рж╛рз▒', 'ржХрз░рж┐', 'ржХрз░рж┐ржЫрж┐рж▓', 'ржХрз░рж┐ржЫрзЗ', 'ржХрз░рж┐ржЫрзЛ', 'ржХрз░рж┐ржЫрзЛржБ', 'ржХрз░рж┐ржм', 'ржХрз░рж┐ржмржЗ', 'ржХрз░рж┐ржмрж▓рзИ', 
        'ржХрз░рж┐ржмрж╛', 'ржХрз░рж┐ржмрж┐', 'ржХрз░рж┐ржо', 'ржХрз░рж┐ржпрж╝рзЗ', 'ржХрз░рж┐ржпрж╝рзЗржЗ', 'ржХрз░рж┐рж▓рзЗ', 'ржХрз░рж┐рж▓рзЗржЗ', 'ржХрз░рж┐рж▓рзЗржУ', 'ржХрз░рж┐рж▓рзЛржБ', 'ржХрз░рзЛ', 
        'ржХрз░рзЛржБ', 'ржХрз░рзЛржБрждрзЗ', 'ржХрз░рзЛржБрждрзЗржЗ', 'ржХрз░рзЛрз▒рж╛', 'ржХрз░рзЛрз▒рж╛ржЗ', 'ржХрз░рзЛрз▒рж╛ржд', 'ржХрз░рзЛрз▒рж╛рз░', 'ржПржЗ', 'ржПржЗржЦрж┐ржирж┐', 'ржПржЗржЬржи', 
        'ржПржЗржЯрзЛ', 'ржПржЗржмрзЛрз░', 'ржПржЗрж╕ржХрж▓', 'ржПржУржБ', 'ржПржУржБрж▓рзЛржХ', 'ржПржирзЗ', 'ржПржирзЗржХрзБрз▒рж╛', 'ржПрз░рж╛', 'ржПрз░рж┐', 'ржУржкрз░ржд', 'ржУрж▓рж╛ржЗ', 
        'ржУрж▓рзЛрз▒рж╛', 'ржХрз░рж╛', 'ржХрз░рж┐', 'ржХрз░рж┐ржм', 'ржХрз░рж┐ржмрж▓рзИ', 'ржХрз░рзЗ', 'ржХрж╛рз░ржгрзЗ', 'ржХрж┐ржирзНрждрзБ', 'ржХрж┐ржпрж╝', 'ржХрзЗрждрж┐ржпрж╝рж╛', 'ржХрзЗрждрж┐ржпрж╝рж╛ржмрж╛', 
        'ржХрзЗрз▒рж▓', 'ржХрзЛржирзЛ', 'ржЧрзИ', 'ржЪрж╛ржЗ', 'ржЪрж╛рж▓рзЗ', 'ржЪрзЛрз▒рж╛', 'ржЫржпрж╝', 'ржЬржи', 'ржЬржирж╛', 'ржЬржирж┐', 'ржЬрзЛржи', 'рждрж╛ржЗ', 'рждрж╛ржХ', 'рждрж╛ржд', 
        'рждрж╛рз░', 'рждрзЗржУржБ', 'рждрзЗржУржБрж▓рзЛржХ', 'рждрзЗрждрж┐ржпрж╝рж╛', 'рждрзЗржирзНрждрзЗ', 'рждрзЛржорж╛рж▓рзЛржХ', 'ржержХрж╛', 'ржерж╛ржХрзЗ', 'ржерж╛ржХрж┐ржм', 'ржжрж┐ржЫрзЗ', 'ржжрж┐ржпрж╝рзЗ', 
        'ржжрж┐ржпрж╝рж╛', 'ржжрж┐рж▓рзЗ', 'ржжрзБржЗ', 'ржжрзБржпрж╝рзЛ', 'ржжрзЗржЦрж╛', 'ржжрзЗржЦрж┐', 'ржиржХрз░рзЗ', 'ржиржХрз░рж┐ржм', 'ржиржХрз░рж┐ржмрж╛', 'ржиржХрз░рж┐рж▓рзЗ', 'ржирждрзБржмрж╛', 
        'ржирж╣ржпрж╝', 'ржирж╛ржЗ', 'ржирж╛ржЗржмрж╛', 'ржирж┐ржЬрз░', 'ржирж┐ржЬрзЗ', 'ржирж┐ржЬрзЗржЗ', 'ржкрз░рж╛', 'ржкрж╛ржБржЪ', 'ржкрж╛ржЗ', 'ржкрж╛ржЫржд', 'ржкрж╛рз░', 'ржкрж╛рз░рзЗ', 
        'ржкрж╛рз░рж┐ржм', 'ржмрзБрж▓рж┐', 'ржмрзЛрж▓рж╛', 'ржмрзЛрж▓рзЗ', 'ржнрж┐рждрз░ржд', 'ржпржжрж┐', 'ржпржжрж┐ржУ', 'ржпрж╛ржУржХ', 'ржпрж╛ржм', 'ржпрж╛ржпрж╝', 'ржпрж╛рз░', 'ржпрж┐ржпрж╝рзЗ', 
        'ржпрж┐рж╕ржХрж▓', 'ржпрзЛрз▒рж╛', 'рж▓ржЧржд', 'рж▓рж╛ржЧрзЗ', 'рж▓рж╛ржЧрж┐ржм', 'рж▓рж╛ржЧрж┐рж▓', 'рж▓рзЛрз▒рж╛', 'рж╢рзЗрж╖ржд', 'рж╕ржХрж▓рзЛ', 'рж╕ржоржпрж╝ржд', 'рж╕рж╛рждрзЗ', 
        'рж╣ржУржХ', 'рж╣ржм', 'рж╣ржмржЗ', 'рж╣ржмрж▓рзИ', 'рж╣рж▓рзЗ', 'рж╣рж▓рзЗржУ', 'рж╣рж▓рзЛ', 'рж╣рж╛рждржд', 'рж╣рж┐ржЪрж╛ржкрзЗ', 'рж╣рзИ', 'рж╣рзИржЫрзЗ', 'рж╣рзИржЫрж┐рж▓', 
        'рж╣рзЛрз▒рж╛', 'рж╣рзЛрз▒рж╛ржЗ', 'рж╣рзЛрз▒рж╛ржд', 'рж╣рзЛрз▒рж╛рз░', 'рж╣ржпрж╝', 'рж╣ржпрж╝рждрзЛ'
    }
    
    # Manipuri stopwords (minimal curated list)
    stopword_dict['manipuri'] = {
        'ъпСъпЧъпи', 'ъпСъпБъпд', 'ъпСъпЧъпгъпЭ', 'ъпСъпЧъпгъпЭъпТъпд', 'ъпСъпЧъпгъпЭъпТъпдъпЧъпЭъпЫ', 'ъпСъпЧъпгъпЭъпБъпи', 'ъпСъпЧъпгъпЭъпЕ', 'ъпСъпЧъпгъпЭъпЕъпБъпи', 'ъпСъпЧъпгъпЭъпЧ', 'ъпСъпЧъпгъпЭъпЧъпБъпи', 
        'ъпСъпЧъпгъпЭъпЧъпТъпд', 'ъпСъпЧъпгъпЭъпЧъпТъпдъпБъпи', 'ъпСъпЧъпгъпЭъпЧъпд', 'ъпСъпЧъпгъпЭъпЧъпи', 'ъпСъпЧъпгъпЭъпЕ', 'ъпСъпЧъпгъпЭъпЕъпБъпи', 'ъпСъпЧъпгъпЭъпЕъпЧъпд', 'ъпСъпЧъпгъпЭъпЕъпЧъпи', 'ъпСъпЧъпгъпЭъпЕъпе', 
        'ъпСъпЧъпгъпЭъпЕъпеъпБъпи', 'ъпСъпЧъпгъпЭъпЕъпеъпЧъпд', 'ъпСъпЧъпгъпЭъпЕъпеъпЧъпи', 'ъпСъпЧъпгъпЭъпБъпд', 'ъпСъпЧъпгъпЭъпБъпи', 'ъпСъпЧъпи', 'ъпСъпЧъпиъпТ', 'ъпСъпЧъпиъпТъпд', 'ъпСъпЧъпиъпТъпиъпЭ', 'ъпСъпЧъпиъпТъпиъпЭъпХ', 
        'ъпСъпЧъпиъпТъпиъпЭъпХъпЧ', 'ъпСъпЧъпиъпТъпиъпЭъпХъпЧъпи', 'ъпСъпЧъпиъпТъпиъпЭъпХъпЕ', 'ъпСъпЧъпиъпТъпиъпЭъпХъпЕъпЧъпд', 'ъпСъпЧъпиъпТъпиъпЭъпХъпЕъпЧъпи', 'ъпСъпЧъпиъпТъпиъпЭъпХъпБъпд', 'ъпСъпЧъпиъпТъпиъпЭъпХъпБъпи', 'ъпСъпЧъпиъпЧ', 
        'ъпСъпЧъпиъпЧъпБъпи', 'ъпСъпЧъпиъпЧъпТъпд', 'ъпСъпЧъпиъпЧъпТъпдъпБъпи', 'ъпСъпЧъпиъпЧъпд', 'ъпСъпЧъпиъпЧъпи', 'ъпСъпЧъпиъпЕ', 'ъпСъпЧъпиъпЕъпБъпи', 'ъпСъпЧъпиъпЕъпЧъпд', 'ъпСъпЧъпиъпЕъпЧъпи', 'ъпСъпЧъпиъпБъпд', 'ъпСъпЧъпиъпБъпи', 
        'ъпСъпЧъпз', 'ъпСъпЧъпзъпЕ', 'ъпСъпЕъпд', 'ъпСъпДъпиъпбъпХ', 'ъпСъпГ', 'ъпСъпГъпБъпиъпб', 'ъпСъпГъпаъпЗ', 'ъпСъпГъпаъпЗъпБъпи', 'ъпСъпГъпаъпЗъпЧ', 'ъпСъпГъпаъпЗъпЧъпБъпи', 'ъпСъпГъпаъпЗъпЕ', 
        'ъпСъпГъпаъпЗъпЕъпБъпи', 'ъпСъпГъпаъпЗъпБъпи', 'ъпСъпГъпЧъпд', 'ъпСъпГъпеъпбъпХ', 'ъпСъпГъпиъпЫ', 'ъпСъпГъпиъпЫъпДъпи', 'ъпСъпГъпиъпЫъпБъпи', 'ъпСъпГъпиъпЫъпНъпЯъпЕ', 'ъпСъпГъпгъпЭ', 'ъпСъпГъпгъпа', 
        'ъпСъпГъпгъпаъпЗ', 'ъпСъпГъпгъпаъпЗъпБъпи', 'ъпСъпГъпгъпаъпЗъпЧ', 'ъпСъпГъпгъпаъпЗъпЧъпБъпи', 'ъпСъпГъпгъпаъпЗъпЕ', 'ъпСъпГъпгъпаъпЗъпЕъпБъпи', 'ъпСъпГъпгъпаъпЗъпБъпи', 'ъпСъпй', 'ъпСъпйъпТъпд', 
        'ъпСъпйъпТъпдъпЧъпЭъпЫ', 'ъпСъпйъпТъпдъпБъпи', 'ъпСъпйъпТъпиъпЭъпХ', 'ъпСъпйъпТъпиъпЭъпХъпЧ', 'ъпСъпйъпТъпиъпЭъпХъпЧъпи', 'ъпСъпйъпТъпиъпЭъпХъпЕ', 'ъпСъпйъпТъпиъпЭъпХъпЕъпЧъпд', 'ъпСъпйъпТъпиъпЭъпХъпЕъпЧъпи', 'ъпСъпйъпТъпиъпЭъпХъпБъпд', 
        'ъпСъпйъпТъпиъпЭъпХъпБъпи', 'ъпСъпйъпБъпи', 'ъпСъпйъпЕ', 'ъпСъпйъпЕъпБъпи', 'ъпСъпйъпЕъпЧъпд', 'ъпСъпйъпЕъпЧъпи', 'ъпСъпйъпЕъпе', 'ъпСъпйъпЕъпеъпБъпи', 'ъпСъпйъпЕъпеъпЧъпд', 'ъпСъпйъпЕъпеъпЧъпи', 'ъпСъпйъпБъпд', 
        'ъпСъпйъпБъпи', 'ъпСъпФъпжъпЭъпХ', 'ъпСъпФъпжъпЭъпХъпЧ', 'ъпСъпФъпжъпЭъпХъпЧъпи', 'ъпСъпФъпжъпЭъпХъпЕ', 'ъпСъпФъпжъпЭъпХъпЕъпЧъпд', 'ъпСъпФъпжъпЭъпХъпЕъпЧъпи', 'ъпСъпФъпжъпЭъпХъпБъпд', 'ъпСъпФъпжъпЭъпХъпБъпи', 'ъпСъпНъпеъпЯъпХ', 
        'ъпСъпНъпеъпЯъпХъпЧ', 'ъпСъпНъпеъпЯъпХъпЧъпи', 'ъпСъпНъпеъпЯъпХъпЕ', 'ъпСъпНъпеъпЯъпХъпЕъпЧъпд', 'ъпСъпНъпеъпЯъпХъпЕъпЧъпи', 'ъпСъпНъпеъпЯъпХъпБъпд', 'ъпСъпНъпеъпЯъпХъпБъпи', 'ъпСъпНъпзъпХ', 'ъпСъпНъпзъпХъпЧ', 
        'ъпСъпНъпзъпХъпЧъпи', 'ъпСъпНъпзъпХъпЕ', 'ъпСъпНъпзъпХъпЕъпЧъпд', 'ъпСъпНъпзъпХъпЕъпЧъпи', 'ъпСъпНъпзъпХъпБъпд', 'ъпСъпНъпзъпХъпБъпи', 'ъпСъпНъпиъпЭ', 'ъпСъпНъпиъпЭъпБъпиъпХ', 'ъпСъпйъпНъпеъпЫ', 'ъпСъпйъпНъпеъпЫъпАъпд', 
        'ъпСъпйъпНъпеъпЫъпАъпдъпЧъпЭъпЫ', 'ъпСъпйъпНъпеъпЫъпАъпдъпБъпи', 'ъпСъпйъпНъпеъпЫъпБъпи', 'ъпСъпйъпНъпеъпЫъпЕ', 'ъпСъпйъпНъпеъпЫъпЕъпБъпи', 'ъпСъпйъпНъпеъпЫъпЕъпЧъпд', 'ъпСъпйъпНъпеъпЫъпЕъпЧъпи', 'ъпСъпйъпНъпеъпЫъпЕъпе', 
        'ъпСъпйъпНъпеъпЫъпЕъпеъпБъпи', 'ъпСъпйъпНъпеъпЫъпЕъпеъпЧъпд', 'ъпСъпйъпНъпеъпЫъпЕъпеъпЧъпи', 'ъпСъпйъпНъпеъпЫъпБъпд', 'ъпСъпйъпНъпеъпЫъпБъпи', 'ъпСъпйъпЦъпгъпб', 'ъпСъпйъпЦъпгъпбъпТъпд', 'ъпСъпйъпЦъпгъпбъпТъпдъпЧъпЭъпЫ', 
        'ъпСъпйъпЦъпгъпбъпТъпдъпБъпи', 'ъпСъпйъпЦъпгъпбъпБъпи', 'ъпСъпйъпЦъпгъпбъпЕ', 'ъпСъпйъпЦъпгъпбъпЕъпБъпи', 'ъпСъпйъпЦъпгъпбъпЕъпЧъпд', 'ъпСъпйъпЦъпгъпбъпЕъпЧъпи', 'ъпСъпйъпЦъпгъпбъпЕъпе', 'ъпСъпйъпЦъпгъпбъпЕъпеъпБъпи', 
        'ъпСъпйъпЦъпгъпбъпЕъпеъпЧъпд', 'ъпСъпйъпЦъпгъпбъпЕъпеъпЧъпи', 'ъпСъпйъпЦъпгъпбъпБъпд', 'ъпСъпйъпЦъпгъпбъпБъпи', 'ъпСъпйъпЗъпе', 'ъпСъпйъпЗъпеъпЧ', 'ъпСъпйъпЗъпеъпЧъпБъпи', 'ъпСъпйъпЗъпеъпЧъпТъпд', 'ъпСъпйъпЗъпеъпЧъпТъпдъпБъпи', 
        'ъпСъпйъпЗъпеъпЧъпд', 'ъпСъпйъпЗъпеъпЧъпи', 'ъпСъпйъпЗъпеъпЕ', 'ъпСъпйъпЗъпеъпЕъпБъпи', 'ъпСъпйъпЗъпеъпЕъпЧъпд', 'ъпСъпйъпЗъпеъпЕъпЧъпи', 'ъпСъпйъпЗъпеъпБъпд', 'ъпСъпйъпЗъпеъпБъпи', 'ъпСъпйъпКъпз', 'ъпСъпйъпКъпзъпЧ', 
        'ъпСъпйъпКъпзъпЧъпБъпи', 'ъпСъпйъпКъпзъпЧъпТъпд', 'ъпСъпйъпКъпзъпЧъпТъпдъпБъпи', 'ъпСъпйъпКъпзъпЧъпд', 'ъпСъпйъпКъпзъпЧъпи', 'ъпСъпйъпКъпзъпЕ', 'ъпСъпйъпКъпзъпЕъпБъпи', 'ъпСъпйъпКъпзъпЕъпЧъпд', 'ъпСъпйъпКъпзъпЕъпЧъпи', 
        'ъпСъпйъпКъпзъпБъпд', 'ъпСъпйъпКъпзъпБъпи', 'ъпСъпБъпд', 'ъпСъпБъпдъпТ', 'ъпСъпБъпдъпТъпд', 'ъпСъпБъпдъпТъпиъпЭ', 'ъпСъпБъпдъпТъпиъпЭъпХ', 'ъпСъпБъпдъпТъпиъпЭъпХъпЧ', 'ъпСъпБъпдъпТъпиъпЭъпХъпЧъпи', 
        'ъпСъпБъпдъпТъпиъпЭъпХъпЕ', 'ъпСъпБъпдъпТъпиъпЭъпХъпЕъпЧъпд', 'ъпСъпБъпдъпТъпиъпЭъпХъпЕъпЧъпи', 'ъпСъпБъпдъпТъпиъпЭъпХъпБъпд', 'ъпСъпБъпдъпТъпиъпЭъпХъпБъпи', 'ъпСъпБъпдъпЧ', 'ъпСъпБъпдъпЧъпБъпи', 
        'ъпСъпБъпдъпЧъпТъпд', 'ъпСъпБъпдъпЧъпТъпдъпБъпи', 'ъпСъпБъпдъпЧъпд', 'ъпСъпБъпдъпЧъпи', 'ъпСъпБъпдъпЕ', 'ъпСъпБъпдъпЕъпБъпи', 'ъпСъпБъпдъпЕъпЧъпд', 'ъпСъпБъпдъпЕъпЧъпи', 'ъпСъпБъпдъпБъпд', 'ъпСъпБъпдъпБъпи', 
        'ъпСъпБъпзъпХ', 'ъпСъпБъпзъпХъпЧ', 'ъпСъпБъпзъпХъпЧъпи', 'ъпСъпБъпзъпХъпЕ', 'ъпСъпБъпзъпХъпЕъпЧъпд', 'ъпСъпБъпзъпХъпЕъпЧъпи', 'ъпСъпБъпзъпХъпБъпд', 'ъпСъпБъпзъпХъпБъпи', 'ъпСъпЗъпй', 'ъпСъпЗъпйъпЧ', 
        'ъпСъпЗъпйъпЧъпБъпи', 'ъпСъпЗъпйъпЧъпТъпд', 'ъпСъпЗъпйъпЧъпТъпдъпБъпи', 'ъпСъпЗъпйъпЧъпд', 'ъпСъпЗъпйъпЧъпи', 'ъпСъпЗъпйъпЕ', 'ъпСъпЗъпйъпЕъпБъпи', 'ъпСъпЗъпйъпЕъпЧъпд', 'ъпСъпЗъпйъпЕъпЧъпи', 'ъпСъпЗъпйъпБъпд', 
        'ъпСъпЗъпйъпБъпи', 'ъпСъпЛъпеъпбъпХ', 'ъпСъпЛъпеъпбъпХъпЧ', 'ъпСъпЛъпеъпбъпХъпЧъпи', 'ъпСъпЛъпеъпбъпХъпЕ', 'ъпСъпЛъпеъпбъпХъпЕъпЧъпд', 'ъпСъпЛъпеъпбъпХъпЕъпЧъпи', 'ъпСъпЛъпеъпбъпХъпБъпд', 'ъпСъпЛъпеъпбъпХъпБъпи', 
        'ъпСъпМъпеъпЭъпХ', 'ъпСъпМъпеъпЭъпХъпЧ', 'ъпСъпМъпеъпЭъпХъпЧъпи', 'ъпСъпМъпеъпЭъпХъпЕ', 'ъпСъпМъпеъпЭъпХъпЕъпЧъпд', 'ъпСъпМъпеъпЭъпХъпЕъпЧъпи', 'ъпСъпМъпеъпЭъпХъпБъпд', 'ъпСъпМъпеъпЭъпХъпБъпи', 'ъпСъпМъпиъпЫ', 
        'ъпСъпМъпиъпЫъпЗ', 'ъпСъпМъпиъпЫъпЗъпБъпи', 'ъпСъпМъпиъпЫъпЗъпТъпд', 'ъпСъпМъпиъпЫъпЗъпТъпдъпБъпи', 'ъпСъпМъпиъпЫъпЗъпд', 'ъпСъпМъпиъпЫъпЗъпи', 'ъпСъпМъпиъпЫъпАъпд', 'ъпСъпМъпиъпЫъпАъпдъпБъпи', 'ъпСъпМъпиъпЫъпЕ', 
        'ъпСъпМъпиъпЫъпЕъпБъпи', 'ъпСъпМъпиъпЫъпЕъпЧъпд', 'ъпСъпМъпиъпЫъпЕъпЧъпи', 'ъпСъпМъпиъпЫъпБъпд', 'ъпСъпМъпиъпЫъпБъпи', 'ъпСъпФъпдъпХ', 'ъпСъпФъпдъпХъпЧ', 'ъпСъпФъпдъпХъпЧъпи', 'ъпСъпФъпдъпХъпЕ', 'ъпСъпФъпдъпХъпЕъпЧъпд', 
        'ъпСъпФъпдъпХъпЕъпЧъпи', 'ъпСъпФъпдъпХъпБъпд', 'ъпСъпФъпдъпХъпБъпи', 'ъпСъпНъпеъпЯъпХ', 'ъпСъпНъпеъпЯъпХъпЧ', 'ъпСъпНъпеъпЯъпХъпЧъпи', 'ъпСъпНъпеъпЯъпХъпЕ', 'ъпСъпНъпеъпЯъпХъпЕъпЧъпд', 'ъпСъпНъпеъпЯъпХъпЕъпЧъпи', 
        'ъпСъпНъпеъпЯъпХъпБъпд', 'ъпСъпНъпеъпЯъпХъпБъпи', 'ъпСъпНъпзъпХ', 'ъпСъпНъпзъпХъпЧ', 'ъпСъпНъпзъпХъпЧъпи', 'ъпСъпНъпзъпХъпЕ', 'ъпСъпНъпзъпХъпЕъпЧъпд', 'ъпСъпНъпзъпХъпЕъпЧъпи', 'ъпСъпНъпзъпХъпБъпд', 
        'ъпСъпНъпзъпХъпБъпи', 'ъпСъпНъпиъпЭ', 'ъпСъпНъпиъпЭъпБъпиъпХ', 'ъпСъпйъпНъпеъпЫ', 'ъпСъпйъпНъпеъпЫъпАъпд', 'ъпСъпйъпНъпеъпЫъпАъпдъпЧъпЭъпЫ', 'ъпСъпйъпНъпеъпЫъпАъпдъпБъпи', 'ъпСъпйъпНъпеъпЫъпБъпи', 'ъпСъпйъпНъпеъпЫъпЕ', 
        'ъпСъпйъпНъпеъпЫъпЕъпБъпи', 'ъпСъпйъпНъпеъпЫъпЕъпЧъпд', 'ъпСъпйъпНъпеъпЫъпЕъпЧъпи', 'ъпСъпйъпНъпеъпЫъпЕъпе', 'ъпСъпйъпНъпеъпЫъпЕъпеъпБъпи', 'ъпСъпйъпНъпеъпЫъпЕъпеъпЧъпд', 'ъпСъпйъпНъпеъпЫъпЕъпеъпЧъпи', 'ъпСъпйъпНъпеъпЫъпБъпд', 
        'ъпСъпйъпНъпеъпЫъпБъпи', 'ъпСъпйъпЦъпгъпб', 'ъпСъпйъпЦъпгъпбъпТъпд', 'ъпСъпйъпЦъпгъпбъпТъпдъпЧъпЭъпЫ', 'ъпСъпйъпЦъпгъпбъпТъпдъпБъпи', 'ъпСъпйъпЦъпгъпбъпБъпи', 'ъпСъпйъпЦъпгъпбъпЕ', 'ъпСъпйъпЦъпгъпбъпЕъпБъпи', 
        'ъпСъпйъпЦъпгъпбъпЕъпЧъпд', 'ъпСъпйъпЦъпгъпбъпЕъпЧъпи', 'ъпСъпйъпЦъпгъпбъпЕъпе', 'ъпСъпйъпЦъпгъпбъпЕъпеъпБъпи', 'ъпСъпйъпЦъпгъпбъпЕъпеъпЧъпд', 'ъпСъпйъпЦъпгъпбъпЕъпеъпЧъпи', 'ъпСъпйъпЦъпгъпбъпБъпд', 'ъпСъпйъпЦъпгъпбъпБъпи'
    }
    
    # Bodo stopwords (currently empty, can be expanded)
    stopword_dict['bodo'] = set()
    
    return stopword_dict

# Clean text by removing punctuation and normalizing spaces
def clean_text(text):
    """Clean text by removing punctuation and normalizing spaces.
    Also performs Unicode normalization for Indic scripts.
    
    Args:
        text (str): Input text to clean.
        
    Returns:
        str: Cleaned text.
    """
    if not text:
        return ""
    
    # Unicode normalization (NFC form)
    text = unicodedata.normalize('NFC', text)
    
    # Convert to lowercase (for English only)
    text = text.lower()
    
    # Remove zero-width joiners and non-joiners
    text = text.replace('\u200C', '').replace('\u200D', '')
    
    # Unify punctuation (Devanagari danda to period)
    text = text.replace('ред', '.')
    
    # Remove ASCII punctuation
    text = re.sub(r'[!"#$%&\'()*+,-./:;<=>?@[\\\]^_`{|}~]', ' ', text)
    
    # Remove Unicode punctuation
    text = re.sub(r'[\u2000-\u206F\u2E00-\u2E7F\u3000-\u303F]', ' ', text)
    
    # Normalize spaces
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# Tokenize text based on language
def tokenize_text(text, lang):
    """Tokenize text based on language with proper handling for Indic scripts.
    
    Args:
        text (str): Input text to tokenize.
        lang (str): Language code ('english', 'hindi', 'assamese', 'manipuri').
        
    Returns:
        list: List of tokens.
    """
    if not text:
        return []
    
    try:
        if lang == 'english':
            # Use NLTK for English
            try:
                return word_tokenize(text)
            except:
                ensure_nltk_data()
                return word_tokenize(text)
        elif lang in ['hindi', 'assamese', 'manipuri', 'bodo']:
            # First normalize the text to NFC form
            text = unicodedata.normalize('NFC', text)
            
            # Use grapheme library if available (best option for Indic scripts)
            if GRAPHEME_AVAILABLE:
                # Split by whitespace and keep grapheme clusters together
                tokens = []
                for word in text.split():
                    # Join graphemes to ensure proper character clusters
                    tokens.append(''.join(grapheme.graphemes(word)))
                logger.info(f"Tokenized with grapheme library: {tokens[:5] if len(tokens) >= 5 else tokens}")
                return tokens
            
            # Fallback to indic_tokenize if available
            if INDIC_NLP_AVAILABLE:
                tokens = indic_tokenize.trivial_tokenize(text)
                logger.info(f"Tokenized with indic_tokenize: {tokens[:5] if len(tokens) >= 5 else tokens}")
                return tokens
            
            # Last resort fallback to regex-based tokenization
            tokens = regex.findall(r'\p{L}+', text)
            logger.info(f"Tokenized with regex: {tokens[:5] if len(tokens) >= 5 else tokens}")
            return tokens
        else:
            # Fallback to simple regex tokenization
            logger.warning(f"Using fallback tokenization for {lang}")
            return re.findall(r'\b\w+\b', text)
    except Exception as e:
        logger.error(f"Error in tokenization: {e}")
        # Fallback to simple regex tokenization
        return re.findall(r'\b\w+\b', text)

# Filter stopwords from tokens
def filter_stopwords(tokens, lang):
    """Filter stopwords from tokens.
    
    Args:
        tokens (list): List of tokens.
        lang (str): Language code ('english', 'hindi', 'assamese', 'manipuri').
        
    Returns:
        list: List of tokens with stopwords removed.
    """
    if not tokens:
        return []
    
    # Get stopwords for the language
    stopword_dict = load_stopwords()
    lang_stopwords = stopword_dict.get(lang, set())
    
    # Filter out stopwords and tokens with length < 2
    filtered_tokens = [token for token in tokens if token not in lang_stopwords and len(token) >= 2]
    
    return filtered_tokens

# Get word frequencies
def get_frequencies(tokens, top_n=20):
    """Get word frequencies.
    
    Args:
        tokens (list): List of tokens.
        top_n (int): Number of top frequencies to return.
        
    Returns:
        list: List of (word, count) tuples.
    """
    if not tokens:
        return []
    
    # Count word frequencies
    word_counts = Counter(tokens)
    
    # Get top N words
    top_words = word_counts.most_common(top_n)
    
    return top_words

# Get the correct font path based on language
def get_font_path(lang):
    """Get the correct font path based on the language.
    
    Args:
        lang (str): Language code.
        
    Returns:
        str: Path to the font file or None.
    """
    font_mapping = {
        'hindi': 'fonts/Noto_Sans_Devanagari/static/NotoSansDevanagari-Regular.ttf',
        'assamese': 'fonts/Noto_Sans_Bengali/static/NotoSansBengali-Regular.ttf',
        'manipuri': 'fonts/Noto_Sans_Meetei_Mayek/static/NotoSansMeeteiMayek-Regular.ttf',
        'bodo': 'fonts/Noto_Sans_Devanagari/static/NotoSansDevanagari-Regular.ttf'
    }
    
    font_path = font_mapping.get(lang)
    
    if font_path and os.path.exists(font_path):
        logger.info(f"Using font: {font_path}")
        return font_path
    
    logger.warning(f"Font for {lang} not found at {font_path}. Using default.")
    return None

# Generate wordcloud image
def generate_wordcloud_image(tokens, lang, width=800, height=400):
    """Generate wordcloud image.
    
    Args:
        tokens (list): List of tokens.
        lang (str): Language code.
        width (int): Width of the wordcloud image.
        height (int): Height of the wordcloud image.
        
    Returns:
        bytes: Image bytes for the wordcloud.
    """
    if not tokens:
        return None
    
    word_freq = Counter(tokens)
    font_path = get_font_path(lang)
    
    try:
        wordcloud = WordCloud(
            width=width,
            height=height,
            background_color='#FDF4DC',
            font_path=font_path,
            min_font_size=10,
            max_font_size=150,
            colormap='copper',
            collocations=False
        )
        
        wordcloud.generate_from_frequencies(word_freq)
        
        img = wordcloud.to_image()
        img_bytes = io.BytesIO()
        img.save(img_bytes, format='PNG')
        img_bytes.seek(0)
        
        logger.info(f"WordCloud generated for {lang} with font: {font_path}")
        return img_bytes
        
    except Exception as e:
        logger.error(f"Error generating wordcloud: {e}")
        return None

# Plot frequency bar chart
def plot_frequency_bar(freq_list, lang='english'):
    """Plot frequency bar chart with proper font handling for Indic scripts.
    
    Args:
        freq_list (list): List of (word, count) tuples.
        lang (str): Language code.
        
    Returns:
        matplotlib.figure.Figure: Matplotlib figure object.
    """
    if not freq_list:
        return None
    
    font_path = get_font_path(lang)
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    words = [item[0] for item in freq_list]
    counts = [item[1] for item in freq_list]
    
    x_pos = range(len(words))
    ax.bar(x_pos, counts, color='#7E6551')
    
    font_prop = None
    if font_path:
        font_prop = fm.FontProperties(fname=font_path)
    
    ax.set_xticks(x_pos)
    ax.set_xticklabels(words, rotation=45, ha='right', fontproperties=font_prop)
    
    ax.set_xlabel('Words', color='#161616', fontproperties=font_prop)
    ax.set_ylabel('Frequency', color='#161616')
    ax.set_title('Word Frequency Distribution', color='#7E6551', fontproperties=font_prop)
    
    plt.yticks(color='#161616')
    plt.tight_layout()
    
    logger.info(f"Bar chart generated for {lang} with font: {font_path}")
    
    return fig

# Setup Streamlit UI
def setup_ui():
    """Setup Streamlit UI."""
    # Set page config
    st.set_page_config(
        page_title="Multilingual WordCloud Generator",
        page_icon="ЁЯУК",
        layout="wide"
    )
    
    # Custom CSS for styling
    st.markdown("""
    <style>
    .main {
        background-color: #FDF4DC;
    }
    h1, h2, h3, h4, h5, h6 {
        color: #7E6551;
    }
    .stButton>button {
        background-color: #7E6551;
        color: white;
    }
    .stTextInput>div>div>input, .stTextArea>div>div>textarea {
        color: white !important;
        background-color: rgba(0, 0, 0, 0.1);
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Header
    st.title("Multilingual WordCloud Generator")
    st.markdown("Generate word clouds and frequency charts for text in multiple languages.")
    
    # Input section
    st.header("Text Input")
    text_input = st.text_area("Enter your text here:", height=150)
    
    # Language selection
    language_options = {
        'english': 'English',
        'hindi': 'Hindi',
        'assamese': 'Assamese',
        'manipuri': 'Manipuri',
        'bodo': 'Bodo'
    }
    selected_lang = st.selectbox("Select language:", list(language_options.keys()), format_func=lambda x: language_options[x])
    
    # Number of top words to display
    top_n = st.slider("Number of top words to display:", min_value=5, max_value=50, value=20)
    
    # Generate button
    generate_button = st.button("Generate")
    
    # Process text when button is clicked
    if generate_button and text_input:
        try:
            # Clean and tokenize text
            original_text = text_input
            cleaned_text = clean_text(text_input)
            tokens = tokenize_text(cleaned_text, selected_lang)
            filtered_tokens = filter_stopwords(tokens, selected_lang)
            
            # Check if we have tokens after filtering
            if not filtered_tokens:
                st.warning("No valid tokens found after filtering. Try a different text or language.")
                return
            
            # Get word frequencies
            freq_list = get_frequencies(filtered_tokens, top_n)
            
            # Create two columns for output
            col1, col2 = st.columns(2)
            
            # Display bar chart in first column
            with col1:
                st.subheader("Word Frequency Chart")
                fig = plot_frequency_bar(freq_list, selected_lang)
                if fig:
                    st.pyplot(fig)
                    # Save figure to a bytes buffer for download
                    buf = io.BytesIO()
                    fig.savefig(buf, format="png")
                    buf.seek(0)
                    st.download_button(
                        label="Download Bar Chart",
                        data=buf,
                        file_name=f"bar_chart_{selected_lang}.png",
                        mime="image/png"
                    )
                else:
                    st.warning("Could not generate frequency chart.")
            
            # Display wordcloud in second column
            with col2:
                st.subheader("Word Cloud")
                wordcloud_bytes = generate_wordcloud_image(filtered_tokens, selected_lang)
                if wordcloud_bytes:
                    st.image(wordcloud_bytes, caption="Word Cloud", use_container_width=True)
                    st.download_button(
                        label="Download WordCloud",
                        data=wordcloud_bytes,
                        file_name=f"wordcloud_{selected_lang}.png",
                        mime="image/png"
                    )
                else:
                    st.warning("Could not generate wordcloud.")

            # Add diagnostics section
            with st.expander("Diagnostics Information"):
                st.subheader("Text Processing Diagnostics")
                
                # Display normalization comparison
                st.write("**Unicode Normalization:**")
                st.write(f"Form used: NFC (Normalization Form Canonical Composition)")
                
                # Display sample of original vs normalized text
                st.write("**Sample Text Comparison:**")
                col1, col2 = st.columns(2)
                with col1:
                    st.write("Original Text (first 100 chars):")
                    st.text(original_text[:100] + ("..." if len(original_text) > 100 else ""))
                with col2:
                    st.write("Normalized Text (first 100 chars):")
                    st.text(cleaned_text[:100] + ("..." if len(cleaned_text) > 100 else ""))
                
                # Display tokenization results
                st.write("**Tokenization Results:**")
                st.write(f"First 20 tokens: {tokens[:20]}")
                
                # Display frequency information
                st.write("**Word Frequencies:**")
                st.write(f"Top {min(20, len(freq_list))} words:")
                freq_df = pd.DataFrame(freq_list[:20], columns=["Word", "Frequency"])
                st.dataframe(freq_df)
                
                # Display font information
                st.write("**Font Information:**")
                font_dir = None
                if selected_lang != 'english':
                    font_dir_mapping = {
                        'hindi': 'fonts/Noto_Sans_Devanagari',
                        'assamese': 'fonts/Noto_Sans_Bengali',
                        'manipuri': 'fonts/Noto_Sans_Meetei_Mayek',
                        'bodo': 'fonts/Noto_Sans_Devanagari'
                    }
                    font_dir = font_dir_mapping.get(selected_lang)
                st.write(f"Language: {selected_lang}")
                st.write(f"Font directory: {font_dir}")
                
                # Technical details
                st.write("**Technical Details:**")
                st.write(f"Total tokens before filtering: {len(tokens)}")
                st.write(f"Total tokens after filtering: {len(filtered_tokens)}")
                st.write(f"Unique words: {len(set(filtered_tokens))}")
                
        except Exception as e:
            st.error(f"Error processing text: {str(e)}")
            logger.error(f"Error processing text: {e}", exc_info=True)
    
    # Display instructions if no text is entered
    elif generate_button and not text_input:
        st.warning("Please enter some text to generate the wordcloud.")

# Main function
def main():
    # Ensure NLTK data is downloaded
    ensure_nltk_data()
    
    # Setup UI
    setup_ui()

# Run the app
if __name__ == "__main__":
    main()